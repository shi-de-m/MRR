---
title: "Partie 1 Projet MRR"
author: "DURAND Lénaïc, SHI DE MILLEVILLE Guillaume"
date: "Binôme n°1, dataset Behavior of the urban traffic of the city of Sao Paulo in Brazil"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(warn=-1)
library(ggplot2)
library(MASS)
library(caret)
library(glmnet)
library(tidyr)
library(dplyr)
library("cowplot")
library("gridExtra")
library(gridExtra)
```


```{r, echo=FALSE}
tab <- read.table("datakronk.csv", header=TRUE, sep=";")
tab[1] = 6.5 + 0.5*tab[1]

X = tab[, -ncol(tab)]
# XLundi = X[1:27,]
# XMardi = X[28:54,]
# XMercredi = X[55:81,]
# XJeudi = X[82:108,]
# XVendredi = X[109:135,]
# X = (XLundi + XMardi + XMercredi + XJeudi + XVendredi)/5


Y = as.data.frame(tab[18])
# YLundi = Y[1:27,]
# YMardi = Y[28:54,]
# YMercredi = Y[55:81,]
# YJeudi = Y[82:108,]
# YVendredi = Y[109:135,]
# Y = (YLundi + YMardi + YMercredi + YJeudi + YVendredi)/5
```

# Première feuille



# Nature du problème

Ce projet est porté sur __l'étude du trafic urbain à Sao Paulo, au Brésil__. Avec l'avancement technologique des dernières années, de nombreuses problématiques touchant les transports et la logistique émergent. Le but est de __trouver un modèle de régression permettant de prédire la lenteur du trafic__ <!--(variable *Slowness_in_traffic*)--> en fonction des paramètres fournis par la base de données. 

La __variable cible est donc *Slowness in traffic*__, nous allons étudier ses variations en fonction des paramètres les plus influents.

# Paramètres

Les paramètres sont les suivants :

-	__*Hour*__ l'heure à laquelle les données sont relevées : arrondie à la demi-heure près, elle prend des valeurs entre 7h et 20h, et les relevés sont faits du Lundi au Vendredi inclus.
-	__*Immobilized_bus*__ le nombre de bus immobilisés
-	__*Broken_Truck*__ le nombre de camions accidentés
-	__*Vehicle_excess*__ le surplus de véhicules
-	__*Accident_victim*__ le nombre de victimes d'accidents
-	__*Running_over*__ le nombre de personnes renversées par un véhicule
-	__*Fire_vehicle*__ le nombre de véhicules de pompier
-	__*Occurence_involving_freight*__ le nombre d'accidents impliquant une cargaison
-	__*Incident_involving dangerous_freight*__ le nombre d'incidents impliquant une cargaison dangereuse
-	__*Lack_of_electricity*__ le niveau d'importance du manque d'électricité
-	__*Fire*__ la présence ou non d'un incendie
-	__*Point_of_flooding*__ le nombre de sources d'inondation
-	__*Manifestations*__ la présence ou non de manifestations
-	__*Defect_in_the_network_of_trolleybuses*__ le nombre de défauts dans le réseau de tramways
-	__*Tree_on_the_road*__ la présence ou non d'arbre(s) sur la route
-	__*Semaphore_off*__ le nombre de feux de circulation ne fonctionnant pas
-	__*Intermittent_semaphore*__ la présence ou non de feux de circulation temporaires


# Influence des paramètres 

<!-- Parmi ces variables, 5 semblent explicitement être reliées à *Slowness_in_traffic* : *Hours*, *Lack_of_electricity*, *Point_of_flooding* et *Semaphore_off*. Nous pouvons le voir sur les figures ci dessous où les variables de  -->

<!-- Cela est moins évident pour d'autres variables comme *Manifestations*. -->

Afin d'exploiter au mieux les données, on calcule la moyenne de la variable de *Slowness_in_traffic* pour toutes les autres variables afin d'en observer une tendance. Une telle manipulation permet de __souligner l'augmentation de *Slowness_in_traffic* aux heures d'affluences__. 
Ces calculs permettent uniquement d'obtenir des observations préliminaires sur les variables. Ainsi, les données qui seront utilisées par la suite seront toujours les données brutes.


```{r, out.width="50%", echo=FALSE}
# plot(data.frame(X[1], Y), main="Slowness_in_traffic en fonction de Hour")
# plot(data.frame(X[10], Y), main="Slowness_in_traffic en fonction de Lack_of_electricity")
# plot(data.frame(X[12], Y), main="Slowness_in_traffic en fonction de Point_of_flooding")
# plot(data.frame(X[13], Y), main="Slowness_in_traffic en fonction de Manifestations")
# plot(data.frame(X[16], Y), main="Slowness_in_traffic en fonction de Semaphore_off")

values = function(column)
{
    res = c()
    for (i in 1:nrow(column))
    {
        if (!(column[i,1] %in% res))
            res = c(res, column[i,1]);
    }
    
    res
}

for (i in c(1))
{
    val = values(X[i])
    grouped_Y = c()
    for (j in 1:length(val))
    {
        m = 0
        n = 0
        for (k in 1:nrow(X))
        {
            if (X[k,i]==val[j])
            {
                m = m + Y[k,1]
                n = n+1
            }
        }
        grouped_Y = c(grouped_Y, m/n)
    }
    plot(data.frame(X[i], Y))
    plot(val, grouped_Y, xlab=colnames(X)[i], ylab="Moyenne de Slowness_in_traffic", ylim=c(min(Y), max(Y)))
}
```
<!-- ```{r} -->
<!-- for (i in (1:ncol(X))) -->
<!-- { -->
<!--     plot(data.frame(X[i], Y)) -->
<!-- } -->
<!-- ``` -->
$$Figure\ 1\ :\ influence \ de \ Hour \  sur \ Slowness \_ in \_ traffic$$

On voit bien qu'aux heures d'affluence il y a une augmentation de la variable *Slowness_in_traffic* puis un pic (augmentation de la variable jusqu'aux pics a 9h et 19h30). __Un tel comportement est attendu__ puisqu'il correspond à la réalité. 

D'autres variables telle que *Semaphore off* ont également un comportement intéressant :

```{r, out.width="50%", echo=FALSE}
# values = function(column)
# {
#     res = c()
#     for (i in 1:nrow(column))
#     {
#         if (!(column[i,1] %in% res))
#             res = c(res, column[i,1]);
#     }
# 
#     res
# }
# 
# for (i in c(10))
# {
#     val = values(X[i])
#     grouped_Y = c()
#     for (j in 1:length(val))
#     {
#         m = 0
#         n = 0
#         for (k in 1:nrow(X))
#         {
#             if (X[k,i]==val[j])
#             {
#                 m = m + Y[k,1]
#                 n = n+1
#             }
#         }
#         grouped_Y = c(grouped_Y, m/n)
#     }
#     plot(data.frame(X[i], Y))
#     plot(val, grouped_Y, xlab=colnames(X)[i], ylab="Moyenne de Slowness_in_traffic", ylim=c(min(Y), max(Y)))
# }
```

<!-- $$Figure\ 2\ :\ influence \ de \ Lack \_of\_electricity\ en\ fonction\ de\ Slowness\ in\ traffic$$ -->

<!-- Slowness in traffic en fonction de Lack of electricity semble avoir un comportement linéaire (nécessite peut-être une transformation avec Arctan)  -->

```{r, out.width="50%", echo=FALSE}

values = function(column)
{
    res = c()
    for (i in 1:nrow(column))
    {
        if (!(column[i,1] %in% res))
            res = c(res, column[i,1]);
    }

    res
}

# for (i in c(1,7,10,12,16))
# for (i in c(1:17)) 
for (i in c(10,12))
{
    val = values(X[i])
    grouped_Y = c()
    for (j in 1:length(val))
    {
        m = 0
        n = 0
        for (k in 1:nrow(X))
        {
            if (X[k,i]==val[j])
            {
                m = m + Y[k,1]
                n = n+1
            }
        }
        grouped_Y = c(grouped_Y, m/n)
    }
    plot(data.frame(X[i], Y))
    plot(val, grouped_Y, xlab=colnames(X)[i], ylab="Moyenne de Slowness_in_traffic", ylim=c(min(Y), max(Y)))
}
```
<!-- $$Figure\ 2\ :\ influence\ de\ Point \_ of \_ flooding\ en\ fonction\ de\ Slowness\ in\ traffic $$ -->
<!-- On calcule une nouvelle fois la moyenne des points de *Slowness in traffic* pour chaque entier *Point of flooding* afin d'en faire ressortir un comportement logarithmique. Après transformation utilisant le log, on peut une nouvelle fois se ramener à un comportement linéaire.  -->

<!-- Plus il y a d'innondations, plus le traffic sera affecté.  -->


```{r, out.width="50%", echo=FALSE}
values = function(column)
{
    res = c()
    for (i in 1:nrow(column))
    {
        if (!(column[i,1] %in% res))
            res = c(res, column[i,1]);
    }

    res
}

for (i in c(16))
{
    val = values(X[i])
    grouped_Y = c()
    for (j in 1:length(val))
    {
        m = 0
        n = 0
        for (k in 1:nrow(X))
        {
            if (X[k,i]==val[j])
            {
                m = m + Y[k,1]
                n = n+1
            }
        }
        grouped_Y = c(grouped_Y, m/n)
    }
    plot(data.frame(X[i], Y))
    plot(val, grouped_Y, xlab=colnames(X)[i], ylab="Moyenne de Slowness_in_traffic", ylim=c(min(Y), max(Y)))
}
```

$$Figure\ 2\ :\ influence\ de \ Semaphore \_ off \_ sur \ Slowness\_ in\_traffic$$

<!-- Encore un comportement linéaire -->

<!-- On calcule une nouvelle fois la moyenne de *Slowness in traffic* pour chaque entier *Semaphore off*, nous permettant de passer a plusieurs nuages de points verticaux à tendance dont il est plus facile d'extraire des informations avant toute régression. -->

On observe alors un __comportement linéaire__ dans le résumé obtenu, tout comme pour la variable *Lack of electricity*.


Certaines variables comme *Incident involving dangerous freight* ou *Occurence involving freight* __ne contiennent presque que des 0__ et ne semblent pas avoir d'influence sur *Slowness in traffic*.
Pour les mêmes raison on retire également *Intermittent Semaphore* et *Fire vehicle*.

*Manifestaton* contient bien moins de 0 mais __ne semble pas influencer *Slowness in traffic*__ puisque la variable cible n'évolue pas pour un nombre grandissant de manifestations. On élimine donc également *Defect in the network of trolley buses*, *Trees on the road*, *Immobilized bus*, *Broken truck*, *Vehicle excess*, *Accident victim*, *Running over* et *Fire*.

L'étude se portera donc sur l'influence de __*Hour*__, __*Lack of electricity*__, __*Point of flooding*__ et __*Semaphore off*__ sur __*Slowness in traffic*__.
<!-- point of flooding n'apparait que lorsque le traffic est lent. Lack of electricity aussi lorsqu'il y a bcp de manque -->



<!-- link between variables to linearize -> Trop de 0 donc on évite les corrélations. Elles vont forcément presque être fortement corrélées à cause de ça d'ailleurs   -->


<!-- Certaines variables ne sont que très peu influentes : -->

<!-- Fire vehicle  -->
<!-- Occurence involving freight -->
<!-- Incident involving dangerous freight -->
<!-- Intermittent Semaphore -->
<!-- Ces variables ne contiennent presque que des 0. -->


<!-- Moyenne slowness lorsqu'il y a des manif ~= moyenne slowness sans manif. De ce fait on retire cette variable -->

<!-- Après observation de la moyenne de Slowness in traffic, on remarque que : -->

<!-- Slowness in traffic en fonction de Semaphore off semble avoir un comportement linéaire -->
<!-- Point of flooding à un comportement linéaire après application du log -->
<!-- Lack of electricity  -->

# Seconde feuille 


<!-- DIFFERENTS MODELES POSSIBLES : OLS, modèle linéaire (résidu gaussien), Ridge (contrainte), Lasso (contrainte absolue) -->

# Introduction

Dans la partie précédente, nous avons présenté le problème et les données utilisées. Nous avions aussi présenté les variables explicatives sur lesquelles nous souhaitions travailler : __*Hour*__, __*Lack of electricity*__, __*Point of flooding*__ et __*Semaphore off*__. Le but de cette partie est d'expliquer la méthodologie suivie pour trouver le meilleur modèle possible.

Tout d'abord, nous allons comparer les variables explicatives obtenues avec un algorithme de sélection à celles que nous avions sélectionnées manuellement. Par exemple avec la méthode __stepwise__, nous trouvons les variables __*Hour*__, __*Lack of electricity*__, __*Point of flooding*__ et __*Manifestations*__, ce qui correspond a 75% à notre analyse en amont. 
Ensuite, nous présenterons un second modèle de classification. 

# Régression

Nous commençons par regarder les résultats donnés par d'autres types de modèles sélectifs, tels que ridge et lasso. Pour cela nous effectuons d'abord une validation croisée pour trouver le paramètre $\lambda$ optimal, c'est-à-dire celui qui minimise l'erreur.

```{r, echo=FALSE}
tab <- read.table("datakronk.csv", header=TRUE, sep=";")
tab[1] = 6.5 + 0.5*tab[1]
# tab[7] = exp(tab[7]*5)
# tab[12] = exp(tab[12]*5)

# tab = as.data.frame(scale(tab))

X = tab[, -ncol(tab)]
Xm = cbind(rep(1,nrow(X)), X)

Y = as.data.frame(tab[18])
Ym = as.matrix(Y)



reg = lm(Slowness_in_traffic~., data=tab)
Y0 = as.matrix(Xm)%*%as.matrix(coef(reg))
g1 <- ggplot(data.frame(Y0, Ym), aes(Y0, Ym)) +
    geom_point(aes(Y0, Ym)) +
    geom_abline(slope=1, intercept=0) +
    ggtitle("                       linear")



ridgeCV = cv.glmnet(x=as.matrix(X), y=as.matrix(Y), alpha=0)
ridge = glmnet(x=as.matrix(X), y=as.matrix(Y), alpha=0, lambda=ridgeCV$lambda.min)
Yr = as.matrix(Xm)%*%as.matrix(coef(ridge))
colnames(Yr) = c()
g2 <- ggplot(data.frame(Yr, Ym), aes(Yr, Ym)) +
    geom_point(aes(Yr, Ym)) +
    geom_abline(slope=1, intercept=0) +
    ggtitle("                       ridge")



lassoCV = cv.glmnet(x=as.matrix(X), y=as.matrix(Y), alpha=1)
lasso = glmnet(x=as.matrix(X), y=as.matrix(Y), alpha=0, lambda=lassoCV$lambda.min)
Yl = as.matrix(Xm)%*%as.matrix(coef(lasso))
colnames(Yl) = c()
g3 <- ggplot(data.frame(Yl, Ym), aes(Yl, Ym)) +
    geom_point(aes(Yl, Ym)) +
    geom_abline(slope=1, intercept=0) +
    ggtitle("                       lasso")



step = step(reg, direction="both", trace=0)
Ys = predict.lm(step)
g4 <- ggplot(data.frame(Ys, Ym), aes(Ys, Ym)) +
    geom_point(aes(Ys, Ym)) +
    geom_abline(slope=1, intercept=0) +
    ggtitle("                       step")




# X2 = tab %>% select(Hour, Lack_of_electricity, Point_of_flooding, Manifestations) #Semaphore_off main, Manifestations step
# X2m = cbind(rep(1,nrow(X2)), X2)
# 
# 
# 
# ridgeCV2 = cv.glmnet(x=as.matrix(X2), y=as.matrix(Y), alpha=0)
# ridge2 = glmnet(x=as.matrix(X2), y=as.matrix(Y), alpha=0, lambda=ridgeCV2$lambda.min)
# Yr2 = as.matrix(X2m)%*%as.matrix(coef(ridge2))
# colnames(Yr2) = c()
# g5 <- ggplot(data.frame(Yr2, Ym), aes(Yr2, Ym)) +
#     geom_point(aes(Yr2, Ym)) +
#     geom_abline(slope=1, intercept=0)
# 
# 
# 
# lassoCV2 = cv.glmnet(x=as.matrix(X2), y=as.matrix(Y), alpha=1)
# lasso2 = glmnet(x=as.matrix(X2), y=as.matrix(Y), alpha=0, lambda=lassoCV2$lambda.min)
# Yl2 = as.matrix(X2m)%*%as.matrix(coef(lasso2))
# colnames(Yl2) = c()
# g6 <- ggplot(data.frame(Yl2, Ym), aes(Yl2, Ym)) +
#     geom_point(aes(Yl2, Ym)) +
#     geom_abline(slope=1, intercept=0)



grid.arrange(g1, g2, g3, g4, ncol=2, nrow = 2) #g5, g6, ncol=3, nrow = 2)

colnames(Ym) = c()
```

Les $R^2$ correspondants aux graphes ci-dessus sont les suivants :

```{r, echo=FALSE}
data.frame(
  Linear_R2 = R2(Y0, Ym),
  Ridge_R2 = R2(Yr, Ym),
  Lasso_R2 = R2(Yl, Ym),
  Step_R2 = R2(Ys, Ym)#,
  #Ridge2_R2 = R2(Yr2, Ym),
  #Lasso2_R2 = R2(Yl2, Ym)
)
```

La RMSE nous donne plus d'information sur l'efficacité de ces modèles. Le $R^2$ varie beaucoup en fonction de la variance et ne peut être interprétée que  difficilement.

```{r, echo=FALSE}
data.frame(
  Linear_RMSE = RMSE(Y0, Ym),
  Ridge_RMSE = RMSE(Yr, Ym),
  Lasso_RMSE = RMSE(Yl, Ym),
  Step_RMSE = RMSE(Ys, Ym)#,
  #Ridge2_RMSE = RMSE(Yr2, Ym),
  #Lasso2_RMSE = RMSE(Yl2, Ym)
  # linear ridge lasso step
)
```

On observe que les résultats des modèles sont assez similaires : un R² d'environ 0,65 et une RMSE d'environ 2,6 (ce qui correspond à une __erreur d'environ 10%__ pour la valeur maximale de *Slowness in trffic* 23,4). Ce sont des __résultats satisfaisants__. Il est cependant surprennant de voir que le meilleur modèle est le modèle de base, c'est-à-dire un modèle linéaire prenant en compte toutes les variables.
Or, la première étude proposait plutôt de ne garder que quelques variables. Afin d'obtenir plus d'information sur ces données, on se propose d'étudier un autre modèle.

# Classification

Cet modèle possible consiste à créer une nouvelle variable binaire Y tel que $Y=1\!\!1_{Slowness\ in\ traffic\ >\ 3^{ème}\ quartile}$, et faire une régression logistique. Ainsi, on cherche un modèle capable de prédire s'il faut prendre sa voiture ou non. On obtient ainsi un __modèle de classification permettant de dire si le trafic est mauvais__ (considéré mauvais si supérieur au $3^{ème}$ quartile.)

On obtient alors la matrice de confusion suivante :

```{r, echo=F}
# Régression logistique

# Slowness_in_traffic
#  Min.   : 3.40      
#  1st Qu.: 7.40      
#  Median : 9.00      
#  Mean   :10.05      
#  3rd Qu.:11.85      
#  Max.   :23.40   

Y_binaire = as.matrix(1*(Y>11.85))


logistique = glm(Y_binaire~., data=X)
Ylog = as.matrix(Xm)%*%as.matrix(coef(logistique))
Ylog = 1*(Ylog>0.5)
conf_mat = table(Y_binaire, Ylog)
conf_mat
```

Ayant obtenu le nombre de faux positifs, faux négatifs, vrais positifs et vrais négatifs, il est aisé d'en déduire les critères d'évaluation suivants :
```{r, echo=F}
TP = conf_mat[2,2]
TN = conf_mat[1,1]
FP = conf_mat[2,1]
FN = conf_mat[1,2]

recall = TP/(TP+FN)
precision = TP/(TP+FP)
F1_score = 2*(precision*recall)/(precision+recall)

data.frame(
  recall = recall,
  precision = precision,
  F1_score = F1_score
)
```


## Conclusion

Afin __d'évaluer les performances__ des modèles de prédiction, nous utiliserons la méthode du __K-fold__. Les perfomances seront __évaluées à l'aide du coefficient de régression et de la somme des résidus__. Au final nous choisirons le modèle donnant le meilleur résultat. Ce modèle servira à prédire avec précision la densité du trafic.

En parallèle, nous disposerons également de notre modèle de classification permettant de dire si le trafic sera très mauvais ou non.

# Préparation de la soutenance 



<!-- DIFFERENTS MODELES POSSIBLES : OLS, modèle linéaire (résidu gaussien), Ridge (contrainte), Lasso (contrainte absolue) -->

# Introduction

Dans la partie précédente, nous avons présenté le problème et les données utilisées. Nous avions aussi présenté les variables explicatives sur lesquelles nous souhaitions travailler : __*Hour*__, __*Lack of electricity*__, __*Point of flooding*__ et __*Semaphore off*__. Le but de cette partie est d'expliquer la méthodologie suivie pour trouver le meilleur modèle possible.

Tout d'abord, nous allons comparer les variables explicatives obtenues avec un algorithme de sélection à celles que nous avions sélectionnées manuellement. Par exemple avec la méthode __stepwise__, nous trouvons les variables __*Hour*__, __*Lack of electricity*__, __*Point of flooding*__ et __*Manifestations*__, ce qui correspond a 75% à notre analyse en amont. 
Ensuite, nous présenterons un second modèle de classification. 

# Régression

Nous commençons par regarder les résultats donnés par d'autres types de modèles sélectifs, tels que ridge et lasso. Pour cela nous effectuons d'abord une validation croisée pour trouver le paramètre $\lambda$ optimal, c'est-à-dire celui qui minimise l'erreur.

```{r, echo=FALSE}
tab <- read.table("datakronk.csv", header=TRUE, sep=";")
tab[1] = 6.5 + 0.5*tab[1]
# tab[7] = exp(tab[7]*5)
# tab[12] = exp(tab[12]*5)

# tab = as.data.frame(scale(tab))

X = tab[, -ncol(tab)]
Xm = cbind(rep(1,nrow(X)), X)

Y = as.data.frame(tab[18])
Ym = as.matrix(Y)



reg = lm(Slowness_in_traffic~., data=tab)
Y0 = as.matrix(Xm)%*%as.matrix(coef(reg))
g1 <- ggplot(data.frame(Y0, Ym), aes(Y0, Ym)) +
    geom_point(aes(Y0, Ym)) +
    geom_abline(slope=1, intercept=0) +
    ggtitle("                       linear")



ridgeCV = cv.glmnet(x=as.matrix(X), y=as.matrix(Y), alpha=0)
ridge = glmnet(x=as.matrix(X), y=as.matrix(Y), alpha=0, lambda=ridgeCV$lambda.min)
Yr = as.matrix(Xm)%*%as.matrix(coef(ridge))
colnames(Yr) = c()
g2 <- ggplot(data.frame(Yr, Ym), aes(Yr, Ym)) +
    geom_point(aes(Yr, Ym)) +
    geom_abline(slope=1, intercept=0) +
    ggtitle("                       ridge")



lassoCV = cv.glmnet(x=as.matrix(X), y=as.matrix(Y), alpha=1)
lasso = glmnet(x=as.matrix(X), y=as.matrix(Y), alpha=0, lambda=lassoCV$lambda.min)
Yl = as.matrix(Xm)%*%as.matrix(coef(lasso))
colnames(Yl) = c()
g3 <- ggplot(data.frame(Yl, Ym), aes(Yl, Ym)) +
    geom_point(aes(Yl, Ym)) +
    geom_abline(slope=1, intercept=0) +
    ggtitle("                       lasso")



step = step(reg, direction="both", trace=0)
Ys = predict.lm(step)
g4 <- ggplot(data.frame(Ys, Ym), aes(Ys, Ym)) +
    geom_point(aes(Ys, Ym)) +
    geom_abline(slope=1, intercept=0) +
    ggtitle("                       step")




# X2 = tab %>% select(Hour, Lack_of_electricity, Point_of_flooding, Manifestations) #Semaphore_off main, Manifestations step
# X2m = cbind(rep(1,nrow(X2)), X2)
# 
# 
# 
# ridgeCV2 = cv.glmnet(x=as.matrix(X2), y=as.matrix(Y), alpha=0)
# ridge2 = glmnet(x=as.matrix(X2), y=as.matrix(Y), alpha=0, lambda=ridgeCV2$lambda.min)
# Yr2 = as.matrix(X2m)%*%as.matrix(coef(ridge2))
# colnames(Yr2) = c()
# g5 <- ggplot(data.frame(Yr2, Ym), aes(Yr2, Ym)) +
#     geom_point(aes(Yr2, Ym)) +
#     geom_abline(slope=1, intercept=0)
# 
# 
# 
# lassoCV2 = cv.glmnet(x=as.matrix(X2), y=as.matrix(Y), alpha=1)
# lasso2 = glmnet(x=as.matrix(X2), y=as.matrix(Y), alpha=0, lambda=lassoCV2$lambda.min)
# Yl2 = as.matrix(X2m)%*%as.matrix(coef(lasso2))
# colnames(Yl2) = c()
# g6 <- ggplot(data.frame(Yl2, Ym), aes(Yl2, Ym)) +
#     geom_point(aes(Yl2, Ym)) +
#     geom_abline(slope=1, intercept=0)



grid.arrange(g1, g2, g3, g4, ncol=2, nrow = 2) #g5, g6, ncol=3, nrow = 2)

colnames(Ym) = c()
```

Les $R^2$ correspondants aux graphes ci-dessus sont les suivants :

```{r, echo=FALSE}
data.frame(
  Linear_R2 = R2(Y0, Ym),
  Ridge_R2 = R2(Yr, Ym),
  Lasso_R2 = R2(Yl, Ym),
  Step_R2 = R2(Ys, Ym)#,
  #Ridge2_R2 = R2(Yr2, Ym),
  #Lasso2_R2 = R2(Yl2, Ym)
)
```

La RMSE nous donne plus d'information sur l'efficacité de ces modèles. Le $R^2$ varie beaucoup en fonction de la variance et ne peut être interprétée que  difficilement.

```{r, echo=FALSE}
data.frame(
  Linear_RMSE = RMSE(Y0, Ym),
  Ridge_RMSE = RMSE(Yr, Ym),
  Lasso_RMSE = RMSE(Yl, Ym),
  Step_RMSE = RMSE(Ys, Ym)#,
  #Ridge2_RMSE = RMSE(Yr2, Ym),
  #Lasso2_RMSE = RMSE(Yl2, Ym)
  # linear ridge lasso step
)
```

On observe que les résultats des modèles sont assez similaires : un R² d'environ 0,65 et une RMSE d'environ 2,6 (ce qui correspond à une __erreur d'environ 10%__ pour la valeur maximale de *Slowness in trffic* 23,4). Ce sont des __résultats satisfaisants__. Il est cependant surprennant de voir que le meilleur modèle est le modèle de base, c'est-à-dire un modèle linéaire prenant en compte toutes les variables.
Or, la première étude proposait plutôt de ne garder que quelques variables. Afin d'obtenir plus d'information sur ces données, on se propose d'étudier un autre modèle.

# Classification

Cet modèle possible consiste à créer une nouvelle variable binaire Y tel que $Y=1\!\!1_{Slowness\ in\ traffic\ >\ 3^{ème}\ quartile}$, et faire une régression logistique. Ainsi, on cherche un modèle capable de prédire s'il faut prendre sa voiture ou non. On obtient ainsi un __modèle de classification permettant de dire si le trafic est mauvais__ (considéré mauvais si supérieur au $3^{ème}$ quartile.)

On obtient alors la matrice de confusion suivante :

```{r, echo=F}
# Régression logistique

# Slowness_in_traffic
#  Min.   : 3.40      
#  1st Qu.: 7.40      
#  Median : 9.00      
#  Mean   :10.05      
#  3rd Qu.:11.85      
#  Max.   :23.40   

Y_binaire = as.matrix(1*(Y>11.85))


logistique = glm(Y_binaire~., data=X)
Ylog = as.matrix(Xm)%*%as.matrix(coef(logistique))
Ylog = 1*(Ylog>0.5)
conf_mat = table(Y_binaire, Ylog)
conf_mat
```

Ayant obtenu le nombre de faux positifs, faux négatifs, vrais positifs et vrais négatifs, il est aisé d'en déduire les critères d'évaluation suivants :
```{r, echo=F}
TP = conf_mat[2,2]
TN = conf_mat[1,1]
FP = conf_mat[2,1]
FN = conf_mat[1,2]

recall = TP/(TP+FN)
precision = TP/(TP+FP)
F1_score = 2*(precision*recall)/(precision+recall)

data.frame(
  recall = recall,
  precision = precision,
  F1_score = F1_score
)
```


## Conclusion

Afin __d'évaluer les performances__ des modèles de prédiction, nous utiliserons la méthode du __K-fold__. Les perfomances seront __évaluées à l'aide du coefficient de régression et de la somme des résidus__. Au final nous choisirons le modèle donnant le meilleur résultat. Ce modèle servira à prédire avec précision la densité du trafic.

En parallèle, nous disposerons également de notre modèle de classification permettant de dire si le trafic sera très mauvais ou non.






```{r, echo=F}
#K-Fold
tab <- read.table("datakronk.csv", header=TRUE, sep=";")
tab[1] = 6.5 + 0.5*tab[1]

Linear_R2 = c()
Ridge_R2 = c()
Lasso_R2 = c()
Step_R2 = c()

Linear_RMSE = c()
Ridge_RMSE = c()
Lasso_RMSE = c()
Step_RMSE = c()


n = nrow(tab)
K = 5

list_index = sample((1:n), n)
len_fold = as.integer(n/K)
folds = matrix(0, K, len_fold)
for (k in (1:K))
{
  fold = list_index[ ((k-1)*len_fold+1) : (k*len_fold)  ]
  folds[k,] = fold
}

for (k in (1:K))
{
  X_k = X[folds[k,],]
  X_km = cbind(rep(1,len_fold), X_k)
  
  Y_k = as.data.frame(tab[18])[folds[k,],]
  Y_km = as.matrix(Y_k)
  
  
  
  data_k = tab[folds[k,],]
  reg = lm(Slowness_in_traffic~., data=data_k)
  tmp = coef(reg)
  tmp[is.na(tmp)] = 0
  Y0 = as.matrix(X_km)%*%as.matrix(tmp)
  
  
  
  ridgeCV = cv.glmnet(x=as.matrix(X_k), y=as.matrix(Y_k), alpha=0)
  ridge = glmnet(x=as.matrix(X_k), y=as.matrix(Y_k), alpha=0, lambda=ridgeCV$lambda.min)
  tmp = coef(ridge)
  tmp[is.na(tmp)] = 0
  Yr = as.matrix(X_km)%*%as.matrix(tmp)
  colnames(Yr) = c()
  
  
  
  lassoCV = cv.glmnet(x=as.matrix(X_k), y=as.matrix(Y_k), alpha=1)
  lasso = glmnet(x=as.matrix(X_k), y=as.matrix(Y_k), alpha=0, lambda=lassoCV$lambda.min)
  tmp = coef(lasso)
  tmp[is.na(tmp)] = 0
  Yl = as.matrix(X_km)%*%as.matrix(tmp)
  colnames(Yl) = c()
  
  
  
  step = step(reg, direction="both", trace=0)
  Ys = predict.lm(step)
  
  Linear_R2 = c(Linear_R2, R2(Y0, Y_km))
  Ridge_R2 = c(Ridge_R2, R2(Yr, Y_km))
  Lasso_R2 = c(Lasso_R2, R2(Yl, Y_km))
  Step_R2 = c(Step_R2, R2(Ys, Y_km))
              
  Linear_RMSE = c(Linear_RMSE, RMSE(Y0, Y_km))
  Ridge_RMSE = c(Ridge_RMSE, RMSE(Yr, Y_km))
  Lasso_RMSE = c(Lasso_RMSE, RMSE(Yl, Y_km))
  Step_RMSE = c(Step_RMSE, RMSE(Ys, Y_km))
}


data.frame(
  moy_Linear_R2 = mean(Linear_R2),
  moy_Ridge_R2 = mean(Ridge_R2),
  moy_Lasso_R2 = mean(Lasso_R2),
  moy_Step_R2 = mean(Step_R2)
)
  
  
data.frame(
  moy_Linear_RMSE = mean(Linear_RMSE),
  moy_Ridge_RMSE = mean(Ridge_RMSE),
  moy_Lasso_RMSE = mean(Lasso_RMSE),
  moy_Step_RMSE = mean(Step_RMSE)
)



boxplot(Linear_R2, Ridge_R2, Lasso_R2, Step_R2, names=c("Linear_R2", "Ridge_R2", "Lasso_R2", "Step_R2"))
boxplot(Linear_RMSE, Ridge_RMSE, Lasso_RMSE, Step_RMSE, names=c("Linear_RMSE", "Ridge_RMSE", "Lasso_RMSE", "Step_RMSE"))
```

